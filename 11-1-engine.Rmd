```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(lidR)

r3dDefaults = rgl::r3dDefaults
m = structure(c(0.921, -0.146, 0.362, 0, 0.386, 0.482, -0.787, 0, 
                -0.06, 0.864, 0.5, 0, 0, 0, 0, 1), .Dim = c(4L, 4L))
rgl::setupKnitr()
r3dDefaults$FOV = 50
r3dDefaults$userMatrix = m
r3dDefaults$zoom = 0.75


knitr::opts_chunk$set(
  comment =  "#>", 
  collapse = TRUE,
  fig.align = "center")
```

# LAScatalog processing engine (1/2) {#engine}

## Problematic

So far we have essentially seen how to process a point cloud after reading a file with `readLAS()`. In practice coverages are made of hundreds or thouthands of files and it is not possible to load them all at once in memory. For example the image below shows a coverage made of 320 files of 1 kmÂ²:

```{r, echo=FALSE}
haliburton <- readRDS("data/ENGINE/haliburton.lascatalog")
projection(haliburton) <- sp::CRS("+proj=utm +zone=17 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
plot(haliburton)
```

This make the extraction of regions of interest (ROIs) such as plot inventories difficult since one must search in which file and sometime which file**s** the ROIs belongs. It also makes the creation of a continuous output such as a continuous DTM, a raster of metrics, individual tree detection or anything else difficult. One may be tempted to loop throught the files (and we have seen many user doing so) like that:

```r
files = list.files("folder/")
for (file in files)
{
  las = readLAS(file)
  output = do.something(las)
  write(output, "path/to/output.ext")
}
```

But this is a very bad practice because each file is loaded without a buffer around it meaning the output will be invalid or weak at the edges of each file because of the absance of spatial context as mentionned several times in the previous chapters of this book. For example let's compute a DTM (see section \@ref(dtm)) using a `for` loop on 4 contiguous files. We can see the obvious invalidy of the DTM between the files but also at the periphery even if it is less obvious to see.


```{r, echo=FALSE, fig.height=7.9, fig.width=8}
dtms = stack("data/ENGINE/DTMs.grd")
plot(dtms[[1]], col = gray.colors(50, 0, 1), legend = FALSE)
```

For comparison the correct DTM is the following and was generated with the lidR's LAScatalog processing engine which have the ability to manage (among other) on-the-fly buffering.

```{r, echo=FALSE, fig.height=7.9, fig.width=8}
plot(dtms[[2]], col = gray.colors(50, 0, 1), legend = FALSE)
```

The `lidR` package provides a powerful and versatile set of tools to work with collection of files and enables to apply any workflow with an automatic mangement of tile buffering, on disk storage, parallelisation of task and so on. While the engine is powerful and versatile it is also a complex tool that needs two chapters to be covered. This chapter presents the main logic and demonstrate how to use the engine for applying common `lidR`'s functions seen in previous chapters accross different examples. The next chapter goes deeper in the engine to demonstrates how developpers can leverage `lidR`'s internals to create their own applications.

The `LAScatalog` class and the `LAScatalog` engine are deeply documented in a two dedicated vignettes available [here](https://cran.r-project.org/web/packages/lidR/vignettes/lidR-LAScatalog-class.html) and [here](). The purpose of this book is to propose an alternative documentation with more illustrated examples and real use cases.

## Read a collection of files

The function `readLAScatalog()` reads the header of all the LAS files of a given folder. The header of a LAS file contains, among other the bouding box of the point cloud. It means that is is possible to know where the file belong without reading the actual payload i.e. without reading the potentially massive amount of data contained in the files. This is what `readLAScatalog()` does. It builds a `SpatialPolygonsDataFrame` out of the bounding boxes of the files.

```r
ctg <- readLAScatalog("path/to/folder/")
```

```{r, echo = FALSE}
ctg = haliburton
```

```{r}
ctg
plot(ctg)
```

In this chapter and the next ones we will use a collection of nine 500 x 500 m files to create examples. The nine file can be downloaded here:

1. [data/ENGINE/catalog/tiles_338000_5238000.laz](tiles_338000_5238000.laz)
2. [data/ENGINE/catalog/tiles_338000_5238500.laz](tiles_338000_5238500.laz)
3. [data/ENGINE/catalog/tiles_338000_5239000.laz](tiles_338000_5239000.laz)
4. [data/ENGINE/catalog/tiles_338500_5238000.laz](tiles_338500_5238000.laz)
5. [data/ENGINE/catalog/tiles_338500_5238500.laz](tiles_338500_5238500.laz)
6. [data/ENGINE/catalog/tiles_338500_5239000.laz](tiles_338500_5239000.laz)
7. [data/ENGINE/catalog/tiles_339000_5238000.laz](tiles_339000_5238000.laz)
8. [data/ENGINE/catalog/tiles_339000_5238500.laz](tiles_339000_5238500.laz)
9. [data/ENGINE/catalog/tiles_339000_5239000.laz](tiles_339000_5239000.laz)

```{r, fig.height=4, fig.width=4}
ctg <- readLAScatalog("data/ENGINE/catalog/")
ctg
plot(ctg)
```

```{r, echo=FALSE}
opt_progress(ctg) <- FALSE
```

## Validating lidar data

Similary to chapter \@ref(io), an important first step in lidar data processing is ensuring that your data is complete and valid. `las_check()` function performs an inspection of `LAScatalog` objects. This function checks the consistancy of the files but does not peforms a deep inspection of each file.

```{r}
las_check(ctg)
```

For a deep (and longer) inspection of each file use `deep = TRUE`. This will load sequentially each file entierly. It is thus important to be sure that yuser have enought memory to manage such inspection.

## Extract a single ROIs

The functions `clip_*()` are a very good starting point for exploring the cabability of the engine. What is presented here with `clip*` function is true with other `lidR`'s functions as we will see in next sections.  `clip_*()` functions allows for extracting a region of interest from a point cloud. For example the following command enable to extract a circle from a point cloud loaded in memory in a `LAS` object.

```r
plot = clip_circle(las, x = 338700, y = 5238650, radius = 15)
```

This can be extended to a `LAScataalog` seamlessly. The engine searches in which file(s) the ROIs belongs and extracts the correponding regions. The output is a `LAS` object.

```{r, rgl = TRUE, fig.height=4, fig.width=4}
roi <- clip_circle(ctg, x = 338700, y = 5238650, radius = 40)
plot(roi, bg = "white", size = 4)
```

### Multiple extractions

Multiple extractions is also possible and is performed the same way by searching the corresponding files and then querying in each files no matter if the region of interest belong in one or several files. The ouput is now a `list` of `LAS` objects.

```{r, fig.height=4, fig.width=4}
x <- c(339348.8, 339141.9, 338579.6, 338520.8, 338110.0, 339385)
y <- c(5239254, 5238717, 5238169, 5239318, 5239247, 5239290)
r <- 40

plot(ctg)
points(x, y)

rois <- clip_circle(ctg, x, y, r)
rois[1:2]
```

### Multiple extractions on disk

No matter which `lidR` function is used, when using a `LAScatalog` as input one option of the engine is to write the generated objects on disk instead of keeping everything in memory. This options can be activated with `opt_output_files() <-` which modify the behavior of the engine. It expects a templated filename so each object will get attributed a different name. In the following example several LAS files will be written on disk with names like `339348.8_5239254_1.las` and the function returns a `LAScatalog` that references all the new files.

```{r, fig.height=4, fig.width=4}
opt_output_files(ctg) <- paste0(tempdir(), "/{XCENTER}_{YCENTER}_{ID}")
rois <- clip_circle(ctg, x, y, r)
rois
plot(rois)
```

We can check the files that were written on disk and see that the names match the template.

```{r}
rois$filename
```

### Multiple extraction with point cloud indexation

Point cloud indexation is a topic covered by [this vignette](https://cran.r-project.org/web/packages/lidR/vignettes/lidR-computation-speed-LAScatalog.html). In short LAS file indexation allows for faster queries when extracting regions of interest in files. Under the hood `lidR` relies on `LASlib` to read files and inherits of the capability to leverage the use of LAX file developed by Martin Isenburg. While extracting hundreds of plots from hundreds of files may take many seconds, the use of index files can speed-up considerably the processing down to few seconds.

### Multiple extractions in parallel

In `lidR` every iterative process can be computed in parallel on a single machine or on multiple machine accessible remotely. Remote parallelisation is a very advanced case not covered in this book for which a [wiki page](https://github.com/Jean-Romain/lidR/wiki/Make-cheap-High-Performance-Computing-to-process-large-datasets) has been written. In the following book only local parallelization is covered.

In `lidR`, parallelization is driven by the package [`future`](https://github.com/HenrikBengtsson/future). Understanding this package is thus a requierement for being able to parallelize task in `lidR`. In the following example we load `future`, we register a parallelization plan and that's it. Extraction is performed in parallel.

```r
library(future)
plan(multisession)
opt_output_files(ctg) <- paste0(tempdir(), "/{XCENTER}_{YCENTER}_{ID}")
rois <- clip_circle(ctg, x, y, r)
```

Notice that in this specific case of extracting 5 plots from indexed files, enabling the parallelization is not relevant because it introduces more overhead that it provides gains.

### Summary

We have learned several thinks about the engine in this section

1. Many functions of the package work the same either with a point cloud (`LAS`) or a collection of files (`LAScatalog`).
2. The behavior of the engine can be modified to write object on disk instead of loading everything in memory. It may be useful for not running out of memory.
3. The engine takes advantage of files indexes
4. Iterative processes can be performed in parallel.

## Ground classification

`classify_ground()` being an important function of `lidR` it also works with a `LAScatalog` seamlessly by processing sequentially pieces of the coverage. Those pieces are called **chunks**. Usually a coverage is already physically split into chunks when the file correspond to a tiling pattern. Thus `classify_ground()`will process each file sequentially. However we have seen in section \@ref(gnd), at the begining of this chapter and all along this book we have seen how important it is to process point clouds including a buffer. This is why the engine loads a buffer on-the-fly around each tiles before to compute anything.

The most basic use is exactly similar to the examples given in section  \@ref(gnd). In this case it is important to tell the engine where to write the outputs. Otherwise the result of each chunk is a classified point cloud stored in memory and after few processed files R will eventually run out of memory and R will crash. The files written on disk being LAS files, the output is a `LAScatalog`

```r
opt_output_files(ctg) <- paste0(tempdir(), "{*}_classified")
classified_ctg <- classify_ground(ctg, csf())
```

In reality R won't crash because the function won't allow to classsify a colelction of file without providing a path to save the outputs of each chunk. Most of `lidR`'s function are safe because they check user inputs.

```{r, error = TRUE}
opt_output_files(ctg) <- ""
classified_ctg <- classify_ground(ctg, csf())
```

### Modify the buffer

It is possible to modify the behavior of the engine by modifing the size of the buffer with `opt_chunk_buffer() <-`. The option `chunk = TRUE` of the function `plot()` allows to see how an option affects the processing pattern. The red boundaries show the pieces (chunks) that will be sequentially processed and the green dotted lines shows the extended regions used for bufferring each chunk.

```{r buffer, fig.show='hold', fig.height=4, fig.width=4}
opt_chunk_buffer(ctg) <- 20
plot(ctg, chunk = TRUE)

opt_chunk_buffer(ctg) <- 50
plot(ctg, chunk = TRUE)
```

Depending on the point-cloud density and the processes applied it might necessary to increase the default buffer. Sometime no buffer at all is need. No matter the size of the buffer, the buffered region it is only useful to get a spatial context that extend the actual size of the chunk. After the processing (here ground classification) the buffer is removed and never returned in memory or saved in files. It is only loaded temporarily for the need of the computation.

### Modify the chunk size

In the example above we have seen that each chunk is actually a file. This is the default behavior because it correspond to the physical splitting pattern. But the engine can actually compute sequentially on any arbitrary pattern. One option of the engine allows to process any arbitrarily sized chunk. This may be useful when each files is too big to be loaded in memory. In this case it might be suitable to reduce the size of each processing region to save memory. Sometime processing more than a file might be useful as well. This is controlled by `opt_chunk_size() <- ` and again the function `plot()` enable to preview the processing pattern.

```{r chunk, fig.show='hold', fig.height=4, fig.width=4}
# Process sequentially tiny 250 x 250 chunks with 10 m buffer
opt_chunk_size(ctg) <- 250
opt_chunk_buffer(ctg) <- 10
plot(ctg, chunk = TRUE)

# Process sequentially tiny 800 x 800 chunks with 40 m buffer
opt_chunk_size(ctg) <- 800
opt_chunk_buffer(ctg) <- 40
plot(ctg, chunk = TRUE)
```

### Parallelize the classification

We have seen in the previous section that parallezation is a property of the engine and thus every functions inherit of parallel processing capabilities. We can combine every known options so far in a valid processing configuration. Doing so does no mean that the classification is performed in parallel. It means that several chunks will be processed simultaneously. So it is important to pay attention to the memory requierements and parallelization is not necessarily relevant in all cases or in all computers.

```r
library(future)
plan(multisession)
opt_chunk_size(ctg) <- 400
opt_chunk_buffer(ctg) <- 40
opt_output_files(ctg) <- paste0(tempdir(), "/{*}_classified")
classified_ctg <- classify_ground(ctg, csf())
```

### Summary

We have learned several thinks about the engine in this section

5. The engine process iteratively some arbitrary region of interest called chunks.
6. Each chunk is loaded with a buffer
7. User can change the chunk size to reduce the amount of memory used if the files are too big.
8. The buffer is removed once the computation is dones. The buffer is used to get rid of edge artifact but is not included in the output.
9. Parallelization is performed by processing several chunks simultaneouly. It means that user need enought processing memory to load several point-cloud at a time.

## Digital Terrain Model

So far we have learned enought about the engine to generate a nice and valid DTMs using `grid_terrain()` (see section \@ref(dtm))

### In memory DTM

Generating a DTM that encompasses a coverage is as easy as generating a DTM that encompasses a single point cloud. The following generates a DTM with the default parameter (i.e. processing by file with a 30 m buffer). In each chunk `grid_terrain()` computes a DTM. The result is thus a collection of raster but one feature of the engine is to merge everything in single manipulable object. A raster is usually light compared to a point cloud so returning everything in memory is allowed if the raster is not too big.

```{r, echo = FALSE}
opt_chunk_size(ctg) <- 0
opt_chunk_buffer(ctg) <- 30
```

```{r, cache = TRUE}
dtm <- grid_terrain(ctg, 2, tin())
```

We can render a shaded DTM to better see the output:

```{r, fig.height=7.9, fig.width=8, cache = TRUE}
dtm_prod <- terrain(dtm, opt = c("slope", "aspect"))
dtm_hillshade <- hillShade(slope = dtm_prod$slope, aspect = dtm_prod$aspect)
plot(dtm_hillshade, col = gray.colors(50, 0 ,1), legend = FALSE)
```

### On disk DTM

When the DTM is too big it is advised to write each chunk on disk. In this case user will have one raster file per chunk. As mentionned above, the buffer is use to perform a valid computation but is removed after the computation and thus there is no overlap between files. In the case of rasters it is possible to build a virtual raster from multiples files to return a lightweigh raster that reference on disk files. This what is returned in this case.

```{r dtmdisk, cache = TRUE}
opt_output_files(ctg) <- opt_output_files(ctg) <- paste0(tempdir(), "/{*}_dtm")
dtm <- grid_terrain(ctg, 1, tin())
dtm
inMemory(dtm)
```

Such light raster can be used later like if it were an in memory raster or a single file raster and is thus much more convenient than having hundred of files on disk without any structure to hold them all. This feature is called Virtual Dataset and is part of the Geospatial Data Abstraction Library (GDAL) (see also [gdalbuildvrt](http://gdal.org/gdalbuildvrt.html)).

### Summary

We have learned several thinks about the engine in this section

10. The engine takes care of returning a single and manipulable object.

## Heigt normalization

There is nothing more to add in this section. Previous section showed everything needed to perform a normalization of the point cloud with `normalize_elevation()` (see section \@ref(normalization)). We created a DTM so can now use it.

```{r normalize, cache = TRUE}
opt_output_files(ctg) <-  paste0(tempdir(), "/{*}_norm")
ctg_norm <- normalize_elevation(ctg, dtm)
```

A point-cloud-based normalization without a raster is also possible

```r
opt_output_files(ctg) <-  paste0(tempdir(), "/{*}_norm")
ctg_norm <- normalize_elevation(ctg, tin())
```

With all the options seen above including the ability to modify the buffer, the chunk size, the name of the files or the parallelization plan.

## Area Based Approach

Area Based Approach (`grid_metrics()`) outputsare raster object like a DTM. The DTM section presented everything needed to compute metrics in an area based approach (see chapter \@ref(aba) showing the main options that can be tunned. It exists however two more options that are not controlable in each function. When reading a LAS file with `readLAS()` two options `select` and `filter` are offered (see chapter \@ref(io)). When processing a collection of files `readLAS()` is called internally and it is not possible to modify these parameters. Instead we can use the options `opt_filter()` and `opt_select()` to propagate these arguments.

```r
opt_select(ctg) <- "xyzci"
opt_filter(ctg) <- "-keep_first"
```

```{r, echo = FALSE}
opt_output_files(ctg_norm) <- ""
opt_progress(ctg_norm) <- FALSE
```

Used in conjonction with `grid_metrics()` it enables to perform the computation only on a selected set of points (first returns for example) loading only the attribute that are actually useful.

```{r, fig.height=7, fig.width=7.1, cache = TRUE}
opt_select(ctg_norm) <- "xyz"
hmean <- grid_metrics(ctg_norm, ~mean(Z), 20)

opt_select(ctg_norm) <- "xyz"
opt_filter(ctg_norm) <- "-keep_first"
hmean <- grid_metrics(ctg_norm, ~mean(Z), 20)

plot(hmean, col = height.colors(50))
```

Not all the function respect these two options. For example it does not make sense to load only XYZ when creating a DTM. The classification of the points is requiered to get the ground points. It is also useless to load the intensity, user data, return number and so on to compute a DTM. The function `grid_terrain()` knows which option to choose and does not respect user inputs. This is the case of several other functions in `lidR`. But when it is suitable user can tne theses options.

### Summary

We have learned several thinks about the engine in this section

11. We can propagate some option to `readLAS()` to optimize and tune our processing
12. Sometime `lidR`s function knows what are the best options and do not respect all the user inputs. This is always documented.

## Individual Tree Detection

At this stage with have all the tools required to find each individual trees using `find_trees()` like with a point cloud but with a collection of files. The following example extracts every tree found over the coverage. The output is returned in memory as a single and continuous object. Each chunk is process with a buffer ensuring that trees will be properly detected at the edges of the files. In thise example 100.000 trees were found.

```{r, echo = FALSE}
opt_chunk_size(ctg_norm) <- 0
opt_chunk_buffer(ctg_norm) <- 30
opt_output_files(ctg_norm) <- ""
```

```{r, cache = TRUE, fig.height=7, fig.width=7}
ttops <- find_trees(ctg_norm, lmf(4))
ttops
plot(ttops, cex = 0.04)
```

However this is not fully satisfying. Each tree is expected to be associated to a single ID but if we count the number of tree with the `ID = 1` we can see that we have 9 trees labbeled "1". Similarly we have 9 trees labelled "2" and so on.

```{r}
sum(ttops$treeID == 1)
```

This error has been volontarily introduced at this stage to illustrate how the engine work. In the engine each chunk is processed **independently** of the others. Potentially they can even be processed on different machines and in a more or less random order. Thus there is no way to know how many trees were found in the first chunk to restart the numbering in the second chunk at the good ID mainly because *'first and second'* chunk may not have any meaning when processing in parallel. The numbering is thus restarted to 1 in each chunk this is why there is 9 trees numbered 1. One in each chunk. When the output is returned in memory this can be easily fixed by reassigning new IDs

```r
ttops$treeID <- 1:length(ttops)
```

But it is more difficult to achieve the same task if each chunk is written in a shapefile and almost impossible to achieve in the case of indiviual tree segmentation as we will see later. To solve this `lidR` introduced two strategies to generate reproducible unique IDs no matter the processing order and the number of tree found. For more details the reader can refer to the documentation. This option can be selected with the parameter `uniqueness`.

```{r, cache = TRUE}
ttops <- find_trees(ctg_norm, lmf(4), uniqueness = "bitmerge")
ttops
```

The attribute `treeID` is now an arbitrary number (here negative but not necessarily) computed from the XY coordinates of the trees that is meaningless but that is guaranteed to be (1) unique (2) reproducible i.e. not matter how the collection of files is processed (by files or by small chunks, in parallel or sequentially, with large or narrow buffer, in memory or on disk) the IDs will always be the same for a given tree.

In all the previous examples we have seen that the engine takes care of merging the intermediate results into a single usable object. When the intermediate results are stored on disk, the final results in R is a single lightweight object such as a `LAScatalog` or a Virtual Datset. This is not true for `find_tree()` and other functions that return `Spatial*` objects. The default behavior is to write in shapefiles and there is no way to build a lightweigth object out of a set of shapefiles. What is returned is thus a vector of written files.

```{r, cache = TRUE}
opt_output_files(ctg_norm) <- paste0(tempdir(), "/{*}")
find_trees(ctg_norm, lmf(4), uniqueness = "bitmerge")
```

### Summary

We have learned several thinks about the engine in this section

13. Each chunk is process strictly independently of the others. 
14. When the intermediate outputs are LAS files, the final output is a lightweight `LAsctalog`. When the intermediate outputs are raster files, the final output is a lightweight Virtual Dataset. However for shapefile and some other datatype this is not alway possible to aggregate the files into a single lightweight object. Thus the names of the files are returned.
15. Strict continuity of the output is not alway trivial because each chunk is processed independently of the others but `lidR` alway guarantee the valididy and the continuity of the outputs.

## Individual Tree Segmentation

Individual tree segmentation with `segment_trees()` is the hardest function to use with a `LAScatalog` because there are many different algorithms. Some of them imply the use of an in memory `RasterLayer` and an in memory `SpatialPointDataFrame`, some require only an in memory `RasterLayer` and some requiere only a point cloud (see section \ref@(its)). Moreover it is very complex to manage the edge artifacts. Imagine there is a tree that belong exctacly between two files. The first half is in one file, the second half in another file. The two files will be processed independently and maybe on different remote computers. Both side of the tree must get attributed with the same ID independently to get something that is strictly continuous. The function `segment_trees()` manage all these points and we will make the demonstraction with the `dalponte2016()` algorithm.

First we create a CHM with a 1 m resolution. This CHM is stored on disk and returned as a light virtual raster. 

```{r, cache = TRUE, fig.height=7, fig.width=7.1}
opt_output_files(ctg_norm) <- paste0(tempdir(), "/chm_{*}")
chm = grid_canopy(ctg_norm, 1, p2r(0.15))
plot(chm, col = height.colors(50))
```

Second we need to compute the seeds by finding the trees as seen above. The result **must be** loaded in memory because there is no way to combine many shapefiles stored on disk like rasters. In this example it is possible because there are 100.000 trees. But for bigger collection it is not possible to apply this algorithm in a simple way.

```r
opt_output_files(ctg_norm) <- ""
ttops <- find_trees(ctg_norm, lmf(4), uniqueness = "bitmerge")
```

To finish we apply `segment_trees()`. Here we don't need to specify any strategy to get unique IDs because the seeds are already labeled with unique IDs. These IDs will be preserved. But for an algorithm without any seed such as `watershed()` it is important to set a strategy.

```{r, cache = TRUE}
opt_output_files(ctg_norm) <- paste0(tempdir(), "/{*}_segmented")
algo <- dalponte2016(chm, ttops)
ctg_segmented <- segment_trees(ctg_norm, algo)
```

The new `LAScatalog` that is returned is made of files that have an extra attribute named `treeID` where each point is labelled with an ID that is unique for each tree even those that belong between two or more files that were processed independently. We can load a plot between two files to check:

```{r, rgl = TRUE}
opt_output_files(ctg_segmented) <- ""
lasplot <- clip_circle(ctg_segmented, 338500, 5238600, 40)
plot(lasplot, color = "treeID", bg = "white", size = 4)
```

We can observe that the segmentation is perfect with respect to the labeling problem. Trees that were segmented twice indendently on each edge were attributed the same IDs on both side of the files independently and thus the final output is a strict wall-to-wall output.

## Retile a catalog

The function `catalog_retile()` allows for retiling a collection of files. This is the best example to combine everything we have seen in this section.

### Retile a catalog into smaller files

`catalog_retile()` allows for retiling the collection of file in tiles of any size.

```{r, fig.height=4, fig.width=4}
opt_output_files(ctg) <- paste0(tempdir(), "/{XLEFT}_{YBOTTOM}")
opt_chunk_buffer(ctg) <- 0
opt_chunk_size(ctg) <- 250
small <- catalog_retile(ctg)
plot(small)
```

### Add a buffer around each files

`catalog_retile()` is a special case where the buffer is not remove after the computation. Indeed the function does not compute anything it only streams point from inputs files to output files. It can be used to add a buffer around each file.

```{r, fig.height=4, fig.width=4}
opt_chunk_buffer(ctg) <- 20
opt_chunk_size(ctg) <- 0
opt_output_files(ctg) <- paste0(tempdir(), "/{ORIGINALFILENAME}_buffered")
buffered <- catalog_retile(ctg)
plot(buffered)
```

Be careful. This may be useful for processing in other softwares but `lidR` loads a buffer on-the-fly and does not support already buffered file. When processing a buffered collection in `lidR` the output is likely to be incorrect with the default parameters.

### Create a new collection with only the first returns

In combination with `opt_filter()`, `catalog_retile()` can be used to generate a new collection of files that contains only first return. This is not very useful in `lidR` because this can be achieve on-the-fly when reading the files but could be useful to process point clouds in other softwares.

```r
opt_chunk_buffer(ctg) <- 0
opt_chunk_size(ctg) <- 0
opt_output_files(ctg) <- paste0(tempdir(), "/{ORIGINALFILENAME}_first")
opt_filter(ctg) <- "-keep_first"
first <- catalog_retile(ctg)
```

### Create a new collection of small and buffered ground returns in parallel

We can combine all the options seen in this chapter to generate a buffered set of tiny files containing only the ground returns. And we can do it in parallel.

```{r, fig.height=4, fig.width=4}
library(future)
plan(multisession)

opt_output_files(ctg) <- paste0(tempdir(), "/{XLEFT}_{YBOTTOM}_first_buffered")
opt_chunk_buffer(ctg) <- 10
opt_chunk_size(ctg) <- 250
opt_filter(ctg) <- "-keep_class 2"
newctg <- catalog_retile(ctg)
plot(newctg)
```

## The case of ground inventories

The case of ground inventories or more genrally independent files is particular. We generated such collection in the first section of this chapter

```{r, echo=FALSE, fig.height=4, fig.width=4}
plot(rois)
```


It is made of circleontinuous files not related to each other. Loading a buffer is meaningless because there is no neighbooring files. Creating chunks is meaningless as well because we usually want one output per file without generating a wall-to-wall object. Worst, with the wrong options the output might be incorrect. For example on the top right of the previous collection we can see two overlapping files. If processed by chunk and with a buffer this will load twice the same point in the overlap plus some extra points from the other plot that are not related at all to the first plot. In short, in the case of ground plot inventory 99% of the cases consist in processing by file, without a buffer and without returning an single aggregated object. This cases actually corresponds to a regular `for` loop as shown in the introduction of this chapter. Thus there is not need of all the feature provided by the `lidR`'s engine and everybody with a small background in programming can write a code that does this job.

However the engine can do it with the appropriated options. In this case the LAScatalog processing engine becomes more or less a parallelisable `for` loop with a real time monitoring.


```{r, fig.height=4, fig.width=4}
opt_chunk_size(rois) <- 0 # processing by files
opt_chunk_buffer(rois) <- 0 # no buffer
opt_wall_to_wall(rois) <- FALSE # disable internal checks to ensure a valid output. Free wheel mode

dtm <- grid_terrain(rois, 1, tin())
plot(dtm[[1]])
```

This can be set in one command

```{r, fig.height=4, fig.width=4}
opt_independent_files(rois) <- FALSE
```

Everything seen so far remains true. But with these options we are sure to do not make mistake when processing independent files.
