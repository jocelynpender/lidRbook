```{r,echo=FALSE,message=FALSE,warning=FALSE}
r3dDefaults = rgl::r3dDefaults
rgl::setupKnitr()
r3dDefaults$FOV = 50
r3dDefaults$zoom = 0.5
library(lidR)
```
# Point Based Approach

In previous sections we have shown that `lidR` provides tools to compute any user-defined metrics using the area-based approach (ABA), voxel-based approach (VBA) and tree-based approach (TBA) Tree Based Approach. Both are well known and commonly applied in a forestry context. In this section were extend concept of metrics to the point-based approach (PBA), which consist of computing user-defined metrics for each point using its neighborhood points. 

For each point, the neighborhood can be either the k nearest neighbors or the points falling within a sphere centered on the point being processed. The following sections present a number of potential applications made possible using the PBA.

## Roof segmentation

[Limberger et al. (2015)](https://doi.org/10.1016/j.patcog.2014.12.020) described a method where flat areas could be classified  in point clouds using eigen value decomposition. Using a user-defined function within `point_metrics()`, we recreate this algorithm to classify roofs in the following urban scene:

```{r, rgl = TRUE}
las <- readLAS("data/chap11/building_WilliamsAZ_Urban_normalized.laz")
plot(las, size = 3)
```

First we need to define a function that computes the eigen value decomposition of a set of points and estimate if the set of point is flat according to the [Limberger et al. (2015)](https://doi.org/10.1016/j.patcog.2014.12.020) criteria.

```r
is.planar = function(x, y, z, th1 = 25, th2 = 6) {
  xyz <- cbind(x,y,z)
  cov_m <- cov(xyz)
  eigen_m <- eigen(cov_m)$value
  is_planar <- eigen_m[2] > (th1*eigen_m[3]) && (th2*eigen_m[2]) > eigen_m[1]
  return(list(planar = is_planar))
}
```

We then apply this function using 20 neighbors, and use `filter = ~Classification != LASGROUND` to ensure that points classified as ground are not processed (The ground is very flat following normalization!!). This provides the added benefit of increasing computation speeds because fewer points are considered.

```r
M <- point_metrics(las, ~is.planar(X,Y,Z), k = 20, filter = ~Classification != LASGROUND)
```

We then merge the output with the point cloud to visualize the result.

```r
las <- add_attribute(las, FALSE, "planar")
las$planar[M$pointID] <- M$planar
plot(las, color = "planar")
```

```{r, echo = FALSE, rgl = TRUE}
# here I'm cheating for the rendering because the example above being too long to compute
options(lidR.progress = FALSE)
las <- readLAS("data/chap11/building_WilliamsAZ_Urban_normalized.laz")
las <- lasdetectshape(las, shp_plane(k = 20), "planar", filter = ~Classification != LASGROUND)
plot(las, color = "planar", size = 3)
```

We can then set a classification to those points

```r
las$Classification[M$pointID] <- ROOF
```

The function `is.planar()` is highly inefficient because `eigen()` is very slow. To remedy this, a dedicated `segment_shape()` function has been added to `lidR` to provide a specialized faster alternative.

```r
las <- segment_shape(las, shp_plane(k = 25), "planar")
```

## Lake and wire segmentation

Eigen value decomposition applied with `point_metrics()` opens the doors to endless possibilities. By tweaking the previous example we can design a lake segmentation algorithm. A lake is a planar region with a vertical normal vector (line perpendicular to the surface). To perform a lake segmentation we need to add these constraints to the `is.planar` function we made above - see `is.linear` below. 

Using the same logic we can also design a wire segmentation algorithm. For a wire we can change the [Limberger et al. (2015)](https://doi.org/10.1016/j.patcog.2014.12.020) constrains to enable the detection of elongated linear features instead of flat ones.

```{r fdefinition}
is.linear = function(x, y, z, th = 10) {
  xyz <- cbind(x,y,z)
  eigen_m <- eigen_values(xyz)
  is_linear <-  th*eigen_m[3] < eigen_m[1] && th*eigen_m[2] < eigen_m[1]
  return(list(linear = is_linear))
}
```

```{r, echo = FALSE, rgl = TRUE}
las <- readLAS("data/chap11/wires.laz", filter = "-keep_random_fraction 0.3")
las <- lasdetectshape(las, shp_line(th1 = 8, k = 15), "linear", filter = ~Classification != LASGROUND)
plot(las, size = 5)
```

```{r, echo = FALSE, rgl = TRUE}
plot(las, color = "linear", size = 5)
```

```{r restore_rgl_zoom, echo = FALSE}
r3dDefaults$zoom = 0.7
```

Again, `lidR` has a dedicated function for this task (using the `"linear"` switch within `segment_shape()`) because we believe it to be important to providing a specialized fast implementation of this tool.

```r
las <- segment_shape(las, shp_line(th = 8, k = 15), "linear")
```

## Multispectral coloring {#sec:pba-mscoloring}

`point_metrics()` is not limited to eigen value related metrics. Here we demonstrate how it can be used to attribute false color to a multispectral point cloud.

Multispectral ALS data are sampled with 3 lasers of differing wavelengths. The point cloud is usually provided in the form of 3 `las` files, each file corresponding to a spectral wavelength. No matter the actual laser wavelength we have attributed first band to be coloured as blue, the second as red, and the third as green to assign a colour to each point. Here we read the 3 `las` files using the `readMSLAS()` function and filter out all points above 300 m elevation.

```{r, rgl = TRUE, message = FALSE}
f1 = "data/chap11/PR1107_c1_ar_c.laz"
f2 = "data/chap11/PR1107_c2_ar_c.laz"
f3 = "data/chap11/PR1107_c3_ar_c.laz"
las <- readMSLAS(f1, f2, f3,  filter = "-keep_z_below 300")
plot(las, color = "ScannerChannel", size = 5)
```

After loading the 3 files, we now want to attribute an RGB value to each point. To assign a single colour to each point we use neighborhood intensity values. In a given neighborhood points occur from all 3 ALS wavelengths, which for our example equate to red, blue and green points. To assign a colour to each point, we average the intensities of point wavelengths in the neighborhood and we consider these 3 values as the RGB values for the central point. Because some points are likely to have 0 red/blue/green neighbors we can set R G and B equal to `NA` for those points and discard those points later.


```{r set_color}
set.color = function(intensity, channel) {
  # Slit the intensities of each channel
  i1 = intensity[channel == 1]
  i2 = intensity[channel == 2]
  i3 = intensity[channel == 3]
  
  # If one channel is missing return RGB = NA
  if (length(i1) == 0 | length(i2) == 0 | length(i3) == 0)
    return(list(R = NA_integer_, G = NA_integer_, B = NA_integer_))
  
  # Average and normalise the intensities
  i1 = as.integer(mean(i1))
  i2 = as.integer(mean(i2))
  i3 = as.integer(mean(i3))
  if (i1 > 255L) i1 = 255L
  if (i2 > 255L) i2 = 255L
  if (i3 > 255L) i3 = 255L
  
  return(list(R = i1, G = i2, B = i3))
}
```

We can then apply this function with `point_metrics()` using a spherical neighborhood radius of `r` equal to 0.5. We then attribute the output of `point_metrics()` back into the `LAS` object for a nice display.

```{r render_rgb, rgl = TRUE}
M = point_metrics(las, ~set.color(Intensity, ScannerChannel), r = 0.5)
las@data$R <- M$R
las@data$G <- M$G
las@data$B <- M$B
colored <- filter_poi(las, !is.na(R)) # remove RGB = NA
plot(colored, color = "RGB", nbits = 8, size = 3)
```

This method is a bit naive 'as is'. First, the intensities returned by each channel are not comparable and require preliminary normalization. We could also argue about the choice of discarding RGB = `NA`. Instead, we could have chosen to set a pure color. To finish, enforcing a maximum value to 255 works in this specific example because very few intensity values are actually above 255 but is meaningless in a general case. 
